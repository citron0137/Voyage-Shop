# **`STEP-12 - Cache`**

> **핵심 요약**: 이 문서는 조회 성능 개선을 위한 캐싱 전략의 설계, 구현 및 테스트 결과를 다룹니다. 데이터셋 규모에 따른 캐시 효율성 검증 결과, 최대 37,000배의 성능 향상을 확인했습니다.

## 1. 캐시 도입 개요

### 1.1 현재 성능 문제점

현재 시스템은 다음과 같은 성능 제한 요소가 존재합니다:

- **반복적인 데이터베이스 조회**: 자주 접근하지만 거의 변경되지 않는 데이터에 대한 불필요한 DB 접근
- **복잡한 집계 쿼리**: 상품 랭킹, 베스트셀러 등 계산 비용이 높은 작업의 실시간 처리
- **트래픽 집중**: 출퇴근 시간, 프로모션 기간 등 특정 시간대 사용자 요청 집중으로 인한 부하
- **확장성 제한**: 데이터베이스 의존도가 높아 사용자 증가에 따른 인프라 비용 급증

### 1.2 캐시 도입 효과 (테스트 결과 요약)

다양한 데이터셋(1k, 10k, 100k)을 대상으로 한 테스트 결과, 캐싱 도입으로 다음과 같은 성과를 확인했습니다:

| 주요 지표 | 성과 |
|---|---|
| 캐시 히트율 | 모든 데이터셋에서 약 99.97% 달성 |
| 응답 시간 | 캐시 히트 시 약 2.5~3.5ms (일관된 성능) |
| 성능 향상 | 데이터 규모에 따라 최대 37,000배 향상 |
| 시스템 안정성 | 모든 테스트에서 0% 오류율 유지 |

> **핵심 발견**: 데이터 규모가 커질수록 캐싱의 효과가 극대화되며, 인덱스 최적화와 결합 시 시스템 안정성이 크게 향상됩니다.

## 2. 캐싱 전략 설계

### 2.1 캐싱 대상 선정

캐싱 적합성 평가 기준에 따라 다음 데이터 유형을 우선적으로 캐싱합니다:

#### 우선 캐싱 대상
- **상품 정보**: 상세 정보, 카테고리, 속성 정보 (읽기 비율 높음, 변경 빈도 낮음)
- **인기 상품 및 추천 상품**: 베스트셀러, 신상품 목록 (계산 비용 높음)
- **카테고리 및 메뉴 구조**: 전체 카테고리 트리 (변경 매우 드묾)
- **검색 결과**: 인기 검색어에 대한 결과 (반복적 동일 검색어)

#### 캐싱 부적합 데이터
- **개인화된 사용자 데이터**: 장바구니, 위시리스트 (변경 빈도 높음)
- **재고 정보**: 실시간 재고 수량 (강한 일관성 필요)
- **결제 정보**: 거래 내역, 결제 상태 (보안 문제)
- **인증 데이터**: 개인 식별 정보 (민감 정보)

### 2.2 캐시 계층 구조

시스템의 성능과 확장성을 위해 다층 캐싱 전략을 채택합니다:

1. **로컬 인메모리 캐시 (L1)** - 애플리케이션 서버 내부
   - 초고속 접근이 필요한 작은 크기 데이터용
   - Caffeine, Guava Cache 등 활용

2. **분산 캐시 (L2)** - 별도 캐시 서버 클러스터
   - 서버 간 공유가 필요한 데이터용
   - Redis를 통한 구현

3. **애플리케이션 프록시 캐시** - API 게이트웨이/CDN
   - 완전히 정적인 콘텐츠용
   - 애플리케이션 서버 부하 완전 제거

### 2.3 캐싱 패턴 적용

데이터 특성에 따라 적절한 캐싱 패턴을 선택합니다:

1. **Cache-Aside (Lazy Loading) 패턴**
   - 적용 대상: 상품 상세 정보, 사용자 설정
   - 작동 방식: 캐시 확인 → 미스 시 DB 조회 → 캐시 저장

2. **Refresh-Ahead 패턴**
   - 적용 대상: 베스트셀러, 추천 상품
   - 작동 방식: 만료 시간 직전에 미리 데이터 갱신

## 3. 현재 구현 상태

### 3.1 구현 완료된 부분

1. **Redis 기반 분산 캐시 (L2)**
   - `CacheConfig` 클래스를 통한 Spring Cache 설정
   - 주문 아이템 순위(OrderItemRank) 데이터 캐싱

2. **Cache-Aside 패턴**
   - 주문 아이템 순위 조회 시 캐시 먼저 확인
   - 캐시 미스 시 계산 후 저장

3. **Cache Stampede 방지**
   - `DistributedLockManager`를 사용한 분산 락 구현
   - 동시 요청의 중복 계산 방지

4. **주기적 캐시 갱신**
   - `@Scheduled` 어노테이션을 통한 10분 주기 갱신

### 3.2 캐시 구현 방법

1. **Redis 캐시 설정**
   - Spring의 `@EnableCaching` 어노테이션으로 캐싱 활성화
   - 캐시별 TTL 설정 (products: 60분, categories: 12시간, bestSellers: 15분)
   - 직렬화/역직렬화 방식 설정

2. **Cache Stampede 방지 전략**
   - **Mutex/Lock 활용**: 캐시 미스 시 첫 요청만 DB 조회, 나머지는 대기
   - **확률적 조기 만료**: TTL 종료 직전에 비동기적 캐시 갱신

### 3.3 향후 구현 예정 사항

1. **로컬 인메모리 캐시 (L1)** - Caffeine 기반 구현
2. **Spring Cache 어노테이션 확대 적용**
3. **고급 캐시 관리 전략** - 동적 TTL, 차등적 Eviction 정책
4. **캐시 모니터링 시스템** - 히트율, 응답 시간 실시간 추적
5. **캐시 예열 메커니즘** - 시스템 재시작 후 자동 로드

## 4. 캐시 관리 전략

### 4.1 TTL(Time-to-Live) 설계

- **실시간성 높은 데이터**: 짧은 TTL (1-5분)
- **일반 콘텐츠**: 중간 TTL (10-60분)
- **거의 변경되지 않는 데이터**: 긴 TTL (1-24시간)

### 4.2 Eviction 정책

- **LRU (Least Recently Used)**: 가장 최근에 사용되지 않은 항목부터 제거
- **TTL 기반**: 만료 시간이 가까운 항목부터 제거
- Redis 권장 설정: `volatile-lru` 또는 `allkeys-lru`

### 4.3 모니터링 지표

- **Hit Ratio**: 캐시 히트율 (목표: 80% 이상)
- **Latency**: 응답 시간 (캐시 히트/미스 구분)
- **Memory Usage**: 메모리 사용량 및 증가 추세
- **Eviction Rate**: 제거 비율 및 패턴

## 5. 부하 테스트 상세 결과

### 5.1 테스트 환경별 결과 비교

| 항목 | 1k 데이터셋 | 10k 데이터셋 | 10k + 인덱스 최적화 | 100k + 인덱스 최적화 |
|------|------------|-------------|-------------------|--------------|
| 캐시 히트율 | 99.97% | 99.97% | 99.21% | 99.97% |
| 캐시 히트 응답 시간 | 3.31ms | 2.57ms | 3.88ms | 2.56ms |
| 캐시 미스 응답 시간 | 89,472.34ms | 146.16ms | 9,557.08ms | 89,593.19ms |
| 성능 향상률 | 약 23,255배 | 약 47배 | 약 10배 | 약 37,000배 |

### 5.2 인덱스 최적화 효과

다음 인덱스 추가로 성능이 크게 향상되었습니다:
```sql
CREATE INDEX idx_orders_created_at ON orders(created_at);
CREATE INDEX idx_order_items_order_product ON order_items(order_id, product_id);
```

- 10k 데이터셋에서 캐시 갱신 후 응답 시간: 3468.05% → 1.12%로 극적 개선
- 캐시 미스 시에도 응답 시간 개선으로 시스템 안정성 향상

### 5.3 캐시 갱신 영향 분석

| 항목 | 1k 데이터셋 | 10k 데이터셋 | 10k + 인덱스 최적화 | 100k + 인덱스 최적화 |
|------|------------|-------------|-------------------|--------------|
| 갱신 전 응답 시간 | 30.91ms | 59.70ms | 250.83ms | 9,591.06ms |
| 갱신 중 응답 시간 | 3.51ms (11.35%) | 3.45ms (5.77%) | 2.67ms (1.07%) | 11,405.39ms (118.92%) |
| 갱신 후 응답 시간 | 2.89ms (9.36%) | 2070.54ms (3468.05%) | 2.81ms (1.12%) | 4,809.82ms (50.15%) |
| 갱신 후 정상화 | 즉시 | 수초 이상 | 즉시 | 부분 회복 |

### 5.4 데이터 규모에 따른 성능 변화

- 초당 처리 요청: 75개(1k) → 58개(10k) → 23개(100k)로 감소
- 캐시 사용 시 응답 시간: 데이터 규모와 무관하게 2.5~3.5ms 유지
- 캐시의 중요성: 데이터 규모 증가에 따라 극대화 (100k 데이터에서 37,000배 성능 향상)

## 6. 결론 및 향후 발전 방향

### 6.1 현재 달성된 성과

- 모든 데이터셋에서 99.97%의 높은 캐시 히트율 달성
- 데이터 규모와 관계없이 약 2.5~3.5ms의 안정적 응답 시간 제공
- 인덱스 최적화와 캐싱 결합으로 시스템 안정성 크게 향상

### 6.2 개선 및 최적화 방향

1. **캐시 갱신 전략 개선**
   - 대규모 데이터셋에서 점진적 캐시 갱신 방식 도입
   - 변경 데이터만 선택적 갱신하는 메커니즘 구현

2. **다층 캐싱 구조 완성**
   - L1(로컬) + L2(분산) 캐시의 단계적 구현
   - 캐시 일관성 확보를 위한 동기화 메커니즘 개발

3. **고급 캐시 관리 기능**
   - 트래픽 패턴에 따른 동적 TTL 조정
   - 선제적 캐시 예열 시스템 구축

### 6.3 추후 구현 단계

1. **1단계(완료)**: 기본 Redis 캐싱 및 Cache-Aside 패턴, 분산 락 기반 Cache Stampede 방지
2. **2단계**: Spring Cache 어노테이션 확대 적용 및 추가 엔티티 캐싱
3. **3단계**: 캐시 모니터링 시스템 구축 및 예열 메커니즘 구현
4. **4단계**: 로컬 캐시(L1) 추가 및 다층 캐싱 전략 구현
5. **5단계**: 고급 캐시 관리 전략 및 최적화 기법 적용

이러한 단계적 접근을 통해 현재의 캐싱 전략을 지속적으로 개선하고, 이상적인 아키텍처에 점진적으로 근접해 나갈 예정입니다. 